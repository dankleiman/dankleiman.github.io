<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>code on Dan Kleiman</title>
    <link>https://dankleiman.com/categories/code/index.xml</link>
    <description>Recent content in code on Dan Kleiman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="/categories/code/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>SQL Quick Tip: Deduping Data with Row Number</title>
      <link>https://dankleiman.com/2019/10/20/sql-quick-tip-deduping-data-with-row-number</link>
      <pubDate>Sun, 20 Oct 2019 07:43:13 -0400</pubDate>
      
      <guid>https://dankleiman.com/2019/10/20/sql-quick-tip-deduping-data-with-row-number</guid>
      <description>Sometimes when you are inspecting data, you come across duplicates that shouldn&amp;rsquo;t exist.
Here&amp;rsquo;s a an easy way to remove duplicate rows using the ROW_NUMBER function.
In a previous post about using ROW_NUMBER to find the latest record for each member in a group, I set up some sample data with this statement:
CREATE TABLE metrics AS ( SELECT date, CASE WHEN n &amp;gt; random() * 2 and n &amp;lt; random() * 4 THEN &#39;A&#39; WHEN n &amp;gt; random() * 2 and n &amp;lt; random() * 4 THEN &#39;B&#39; WHEN n &amp;gt; random() * 4 and n &amp;lt; random() * 6 THEN &#39;C&#39; WHEN n &amp;gt; random() * 6 and n &amp;lt; random() * 8 THEN &#39;D&#39; WHEN n &amp;gt; random() * 8 and n &amp;lt; random() * 10 THEN &#39;E&#39; ELSE &#39;F&#39; END as metric, round(random() * 1000) as value FROM generate_series(1,11,1) as n JOIN ( SELECT day + interval &#39;1 day&#39; * round(random()*100) as date FROM generate_series(&#39;2019-01-01&#39;, &#39;2019-01-31&#39;, interval &#39;1 day&#39;) day ) d ON true );  While that statement is a great way to get a bunch of random sample data to play with, there is no guarantee that the values or the dates will be unique.</description>
    </item>
    
    <item>
      <title>SQL Quick Tip: Find the Latest Record for Each Member of a Group</title>
      <link>https://dankleiman.com/2019/10/12/sql-quick-tip-find-the-latest-record-for-each-member-of-a-group</link>
      <pubDate>Sat, 12 Oct 2019 07:30:37 -0400</pubDate>
      
      <guid>https://dankleiman.com/2019/10/12/sql-quick-tip-find-the-latest-record-for-each-member-of-a-group</guid>
      <description>In this post, we&amp;rsquo;re going to look at a techinque for finding the lastest full record for each member of a group.
Now, it might not be obvious why this is an annoying problem, but I see people back into having this problem from two different directions.
First, you know how to find the MAX date for each member in a group:
 metric | latest_metric --------+------------------------ A | 2019-04-30 00:00:00-04 B | 2019-05-08 00:00:00-04 C | 2019-05-08 00:00:00-04 D | 2019-05-08 00:00:00-04 E | 2019-05-08 00:00:00-04 F | 2019-05-08 00:00:00-04 (6 rows)  But you need additional column data from the same row as that most recent date.</description>
    </item>
    
    <item>
      <title>SQL Quick Tip: Find Missing Data</title>
      <link>https://dankleiman.com/2019/10/06/sql-quick-tip-find-missing-data</link>
      <pubDate>Sun, 06 Oct 2019 21:54:42 -0400</pubDate>
      
      <guid>https://dankleiman.com/2019/10/06/sql-quick-tip-find-missing-data</guid>
      <description>Whenever you have two sets of data and you need to find the entries that are in your first set, but not in your second set, use this pattern.
Let&amp;rsquo;s say you have an application that tracks user sign-ups separately for user sign-ins. You might be interested in knowing which users have signed up, but never signed in.
Postgres makes it easy to mock up some sample data so we can work through this use case.</description>
    </item>
    
    <item>
      <title>SQL Quick Tip: Showing Changes in Your Data</title>
      <link>https://dankleiman.com/2019/09/27/sql-quick-tip-showing-changes-in-your-data</link>
      <pubDate>Fri, 27 Sep 2019 00:00:00 -0400</pubDate>
      
      <guid>https://dankleiman.com/2019/09/27/sql-quick-tip-showing-changes-in-your-data</guid>
      <description>In this tip, we want to look at a concise way to shows changes in your data.
I tend to think of this type of problem as a going from &amp;ldquo;finding&amp;rdquo; data to &amp;ldquo;describing&amp;rdquo; data. For example, if you know how to get every value for a user in the database for the last 30 days, then you can &amp;ldquo;find&amp;rdquo; data. When you calculate aggregates of that data using functions like MAX, MIN, SUM, or AVG, you are now &amp;ldquo;describing&amp;rdquo; the data.</description>
    </item>
    
    <item>
      <title>SQL Quick Tip: Guarantee Rows for Every Date in Your Report</title>
      <link>https://dankleiman.com/2019/09/12/sql-quick-tip-guarantee-rows-for-every-date-in-your-report</link>
      <pubDate>Thu, 12 Sep 2019 21:57:02 -0400</pubDate>
      
      <guid>https://dankleiman.com/2019/09/12/sql-quick-tip-guarantee-rows-for-every-date-in-your-report</guid>
      <description>&lt;p&gt;When you are reporting on metrics over time, sometimes your data will have missing entries on certain days.&lt;/p&gt;

&lt;p&gt;In these cases, it&amp;rsquo;s useful to be able to ensure that every date shows up in your report, regardless of whether or not there is a metric in the dataset for that date.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s use daily user logins to a website for a reporting metric to illustrate how you solve this problem.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>SQL Quick Tip: Present Cleaner Results with Custom Ordering</title>
      <link>https://dankleiman.com/2019/08/30/sql-quick-tip-present-cleaner-results-with-custom-ordering</link>
      <pubDate>Fri, 30 Aug 2019 19:40:40 -0400</pubDate>
      
      <guid>https://dankleiman.com/2019/08/30/sql-quick-tip-present-cleaner-results-with-custom-ordering</guid>
      <description>&lt;p&gt;Usually, when you add an &lt;code&gt;ORDER BY&lt;/code&gt; clause to your SQL query, you want to sort by your columns&amp;rsquo; values.&lt;/p&gt;

&lt;p&gt;To track the top 10 cryptocurrencies by price over the last 90 days, for example, you would write a query like this:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Roll Your Own Database: Part 1</title>
      <link>https://dankleiman.com/2019/02/16/roll-your-own-database-part-1</link>
      <pubDate>Sat, 16 Feb 2019 11:31:03 -0500</pubDate>
      
      <guid>https://dankleiman.com/2019/02/16/roll-your-own-database-part-1</guid>
      <description>Warning: This post is NSFW. In this series, we are going to build a really, really simple database management system that you should by no means use in a production work environment.
Here&amp;rsquo;s the experiment:
 Start with a naive implementation of a database &amp;ndash; read and write from a local csv file Reach for all the normal features we use every day: basic CRUD, aggregate queries, complex joins Realize that our basic implementation falls short Look at how these problems are solved in modern systems  Think of the whole series as one giant experiment in Cunningham&amp;rsquo;s Law: &amp;ldquo;the best way to get the right answer on the internet is not to ask a question; it&amp;rsquo;s to post the wrong answer.</description>
    </item>
    
    <item>
      <title>Seed Data for Your WIP SQL Queries</title>
      <link>https://dankleiman.com/2019/02/05/seed-data-for-your-wip-sql-queries</link>
      <pubDate>Tue, 05 Feb 2019 20:41:59 -0500</pubDate>
      
      <guid>https://dankleiman.com/2019/02/05/seed-data-for-your-wip-sql-queries</guid>
      <description>In my last post, I wrote about steps you can take to make writing complicated queries more manageable. One aspect that I didn&amp;rsquo;t cover in that post is how to set sample data to work with during the writing process. Assuming you&amp;rsquo;re not working directly in your production database as you test out new queries (right? right?? right???), you need some way to work on your new ideas.
In this post, I want to share some tips and tricks for creating reliable, reproduceable test data to help you develop new ideas in SQL.</description>
    </item>
    
    <item>
      <title>Stop Writing SQL Backwards</title>
      <link>https://dankleiman.com/2019/01/02/stop-writing-sql-backwards</link>
      <pubDate>Wed, 02 Jan 2019 09:21:26 -0500</pubDate>
      
      <guid>https://dankleiman.com/2019/01/02/stop-writing-sql-backwards</guid>
      <description>How many times have you started off building a complicated analytical SQL query like this?
SELECT . . . . uh??? . . SELECT * FROM . . . SELECT . . .  And you get stuck trying to figure out exactly what you want to select. You&amp;rsquo;re thinking about averages, group by&amp;rsquo;s, the order of your results or some change you want to see over time and the query editor is just sitting there, taunting you, because in SQL, you have to know up front what you want to select into your final results.</description>
    </item>
    
    <item>
      <title>3 Ways to Level Up Your SQL as a Software Engineer</title>
      <link>https://dankleiman.com/2018/02/06/3-ways-to-level-up-your-sql-as-a-software-engineer</link>
      <pubDate>Tue, 06 Feb 2018 12:31:44 -0500</pubDate>
      
      <guid>https://dankleiman.com/2018/02/06/3-ways-to-level-up-your-sql-as-a-software-engineer</guid>
      <description>&lt;p&gt;If you are a software engineer and you have just enough SQL to write queries that count, sum, average join and maybe sub-select, then I&amp;rsquo;m writing this post for you. If, when you need more complicated analysis or computation, you pull your query results into excel or your favorite scripting language to do more processing, then I have some good news.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a whole lot more you can do right in SQL and it&amp;rsquo;s not too bad to learn how to do it.&lt;/p&gt;

&lt;p&gt;In this post, I&amp;rsquo;m going to cover a few concepts that have recently helped me do more computation, better analysis, and it turns out, more efficient querying&amp;hellip;and I hope they help you too.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Keeping an Engineering Notebook</title>
      <link>https://dankleiman.com/2018/01/28/keeping-an-engineering-notebook</link>
      <pubDate>Sun, 28 Jan 2018 22:52:26 -0500</pubDate>
      
      <guid>https://dankleiman.com/2018/01/28/keeping-an-engineering-notebook</guid>
      <description>&lt;p&gt;The best upgrade I&amp;rsquo;ve made to my workflow in the past year was to start keeping an engineering notebook.&lt;/p&gt;

&lt;p&gt;Whenever I start a new project, the first thing I do is create a new section in my engineering notebook. It&amp;rsquo;s really simple. With a tiny script, I generate a dedicated folder for the new project, plus three sub-folders and a README:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    some_new_work
        |__ notes/
        |__ data/
        |__ scripts/
        |__ README.md
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;As simple as this structure seems, it has had a tremendous impact on my work. In this post, I want to try to unpack why and how I think that&amp;rsquo;s working.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>More Efficient Solutions to the Top N per Group Problem</title>
      <link>https://dankleiman.com/2017/11/07/more-efficient-solutions-to-the-top-n-per-group-problem</link>
      <pubDate>Tue, 07 Nov 2017 20:46:05 -0500</pubDate>
      
      <guid>https://dankleiman.com/2017/11/07/more-efficient-solutions-to-the-top-n-per-group-problem</guid>
      <description>&lt;p&gt;In my last post, I tried to tackle getting &lt;a href=&#34;https://dankleiman.com/blog/2017/10/30/top-n-per-group-in-bigquery/&#34;&gt;the Top N Results per Group&lt;/a&gt; from a BigQuery dataset.&lt;/p&gt;

&lt;p&gt;When I tweeted out the post, I got some great feedback and suggestions for more efficient ways to get the same results, so in this post I want to try to understand &lt;em&gt;why&lt;/em&gt; the alternatives are more efficient.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Top N Per Group in BigQuery</title>
      <link>https://dankleiman.com/2017/10/30/top-n-per-group-in-bigquery</link>
      <pubDate>Mon, 30 Oct 2017 19:29:32 -0400</pubDate>
      
      <guid>https://dankleiman.com/2017/10/30/top-n-per-group-in-bigquery</guid>
      <description>&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; After I posted this initially, I got some &lt;a href=&#34;https://twitter.com/Dan_Kleiman/status/925921880397287425&#34;&gt;great&lt;/a&gt; &lt;a href=&#34;https://www.reddit.com/r/bigquery/comments/7aecfe/top_n_per_group_in_bigquery/&#34;&gt;feedback&lt;/a&gt;, so I wrote a follow-up post &lt;a href=&#34;https://dankleiman.com/2017/11/07/more-efficient-solutions-to-the-top-n-per-group-problem/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post, we are going to explore a strategy for collecting the &lt;strong&gt;Top N results per Group&lt;/strong&gt; over a mixed dataset, all in a single query.&lt;/p&gt;

&lt;p&gt;I stumbled onto this solution the other day, mostly driven by the fear that I was re-scanning my BigQuery data too often. At the time, the only way I knew how to look at a Top 10 list of a subset of the data was to add a &lt;code&gt;WHERE&lt;/code&gt; clause limiting the whole data set to a single group and combine with &lt;code&gt;ORDER BY&lt;/code&gt; and &lt;code&gt;LIMIT&lt;/code&gt; clauses.&lt;/p&gt;

&lt;p&gt;For each group, I would just modify the &lt;code&gt;WHERE&lt;/code&gt; clause, rescan all the data, and get new results. I thought there had to be an easier way to get the same ordered subset for any particular group in the data, all at once.&lt;/p&gt;

&lt;p&gt;It turns out, there is a much more efficient way to solve this problem.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Don&#39;t Blow Your BigQuery Budget on Unknown Data!</title>
      <link>https://dankleiman.com/2017/10/06/dont-blow-your-bigquery-budget-on-unknown-data</link>
      <pubDate>Fri, 06 Oct 2017 14:55:37 -0400</pubDate>
      
      <guid>https://dankleiman.com/2017/10/06/dont-blow-your-bigquery-budget-on-unknown-data</guid>
      <description>&lt;p&gt;It&amp;rsquo;s easy to blow your BigQuery budget when you are exploring a new data set. Because you&amp;rsquo;re billed for the amount of data scanned, not the ultimate result set, when you don&amp;rsquo;t know what you&amp;rsquo;re looking  for, you can end up with wasteful queries.&lt;/p&gt;

&lt;p&gt;In this post, I&amp;rsquo;m going to share some tips for more efficiently scanning data in BigQuery when you don&amp;rsquo;t quite know what you need.
&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GoBridge with Bill Kennedy</title>
      <link>https://dankleiman.com/2017/02/12/gobridge-with-bill-kennedy</link>
      <pubDate>Sun, 12 Feb 2017 15:18:04 -0500</pubDate>
      
      <guid>https://dankleiman.com/2017/02/12/gobridge-with-bill-kennedy</guid>
      <description>&lt;p&gt;Last weekend, I had the chance to volunteer at a GoBridge event taught by Bill Kennedy of Ardan Labs. I&amp;rsquo;m trying to make &lt;a href=&#34;https://dankleiman.com/blog/2016/12/29/my-5-strategies-for-learning-go-in-2017/&#34;&gt;2017 my year of learning Go&lt;/a&gt;, so helping out at the event felt like a natural extension and a great way to connect with more people in the Go community.&lt;/p&gt;

&lt;p&gt;Going in with Ruby as my first language, I braced myself for static typing and wanted concurrent programming to bend my brain, but that&amp;rsquo;s not really what happened at all.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>