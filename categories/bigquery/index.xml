<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bigquery on Dan Kleiman</title>
    <link>http://dankleiman.com/categories/bigquery/index.xml</link>
    <description>Recent content in bigquery on Dan Kleiman</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="http://dankleiman.com/categories/bigquery/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>More Efficient Solutions to the Top N per Group Problem</title>
      <link>http://dankleiman.com/2017/11/07/more-efficient-solutions-to-the-top-n-per-group-problem</link>
      <pubDate>Tue, 07 Nov 2017 20:46:05 -0500</pubDate>
      
      <guid>http://dankleiman.com/2017/11/07/more-efficient-solutions-to-the-top-n-per-group-problem</guid>
      <description>&lt;p&gt;In my last post, I tried to tackle getting &lt;a href=&#34;http://dankleiman.com/blog/2017/10/30/top-n-per-group-in-bigquery/&#34;&gt;the Top N Results per Group&lt;/a&gt; from a BigQuery dataset.&lt;/p&gt;

&lt;p&gt;When I tweeted out the post, I got some great feedback and suggestions for more efficient ways to get the same results, so in this post I want to try to understand &lt;em&gt;why&lt;/em&gt; the alternatives are more efficient.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;first-attempt-using-row-number&#34;&gt;First Attempt using ROW_NUMBER&lt;/h2&gt;

&lt;p&gt;Here&amp;rsquo;s the initial query I was using:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT
  subreddit,
  author,
  author_count,
  total_score,
  comment_rank
FROM (
  SELECT
    subreddit,
    author,
    SUM(score) AS total_score,
    ROW_NUMBER() OVER (PARTITION BY subreddit ORDER BY SUM(score) DESC) AS comment_rank,
    COUNT(DISTINCT author) OVER (PARTITION BY subreddit) AS author_count
  FROM
    `fh-bigquery.reddit_comments.2015_07`
  WHERE
    author NOT IN (&#39;[deleted]&#39;,
      &#39;AutoModerator&#39;)
  GROUP BY
    1,
    2 )
WHERE
  comment_rank &amp;lt;= 10
  AND author_count &amp;gt; 9
ORDER BY
  1,
  5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;For the Top N problem, the key is computing the rank of each author&amp;rsquo;s scores, relative to the others in any particular group. In the query above, that&amp;rsquo;s this line:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ROW_NUMBER() OVER (PARTITION BY subreddit ORDER BY SUM(score) DESC) AS comment_rank,
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The partition isolates the group, in this case a single subreddit. Then, you can order the authors by score within each group. The &lt;code&gt;ROW_NUMBER&lt;/code&gt; function is a way to capture an ordered list of those partitioned ranks.&lt;/p&gt;

&lt;p&gt;We use the ordered list position later in the main select as part of the &lt;code&gt;WHERE&lt;/code&gt; clause to only take author&amp;rsquo;s whose subreddit-specific comment rank is in the top 10.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s what the query plan looks like for our &lt;code&gt;ROW_NUMBER&lt;/code&gt; query:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dankleiman.com/img/row_number_query_plan.png&#34; alt=&#34;BigQuery ROW_NUMBER query plan&#34;&gt;&lt;/p&gt;

&lt;h2 id=&#34;using-array-agg&#34;&gt;USING ARRAY_AGG&lt;/h2&gt;

&lt;p&gt;The next query that I wanted to try was suggested by &lt;a href=&#34;https://twitter.com/ElliottBrossard&#34;&gt;Elliot Brossard&lt;/a&gt; who actually works on BigQuery at Google. And, if I had been paying better attention when I pulled my first pass solution from StackOverflow, I would have seen that he also made &lt;a href=&#34;https://stackoverflow.com/a/44680685&#34;&gt;this suggestion on the original post&lt;/a&gt;. Thanks for your patience, Elliot!&lt;/p&gt;

&lt;p&gt;Elliot suggests using &lt;code&gt;ARRAY_AGG&lt;/code&gt; with a limit to get a set of ranked results instead of using the &lt;code&gt;ROW_NUMBER&lt;/code&gt; function:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT
  subreddit,
  ARRAY_AGG(STRUCT(author,
      total_score)
  ORDER BY
    total_score DESC
  LIMIT
    10) AS top_authors
FROM (
  SELECT
    subreddit,
    author,
    SUM(score) AS total_score,
    COUNT(DISTINCT author) OVER (PARTITION BY subreddit) AS author_count
  FROM
    `fh-bigquery.reddit_comments.2015_07`
  WHERE
    author NOT IN (&#39;[deleted]&#39;,
      &#39;AutoModerator&#39;)
  GROUP BY
    1,
    2 )
WHERE
  author_count &amp;gt; 9
GROUP BY
  subreddit
ORDER BY
  1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#array_agg&#34;&gt;ARRAY_AGG&lt;/a&gt; gives you a ton of power to condense parts of what we did in the first query.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;you can pull a subset of data into a single array, in this case &lt;code&gt;STRUCT(author, total_score)&lt;/code&gt;, which gives you pretty much what we used &lt;code&gt;PARTITION&lt;/code&gt; for in the first query&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ARRAY_AGG&lt;/code&gt; takes &lt;code&gt;ORDER BY&lt;/code&gt; and &lt;code&gt;LIMIT&lt;/code&gt; clauses, which together gives us the &amp;ldquo;top N&amp;rdquo; piece&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Not only that, but the result set is reduced by an order of magnitude. The first query returns 196,970 results, or 10 rows per subreddit. But the &lt;code&gt;ARRAY_AGG&lt;/code&gt; query only has 19,697 results because we only need one row per subreddit &amp;ndash; the authors and there comment scores, per the &lt;code&gt;STRUCT&lt;/code&gt; we defined in the &lt;code&gt;SELECT&lt;/code&gt; statement are a single array value column.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results using ROW_NUMBER&lt;/strong&gt;
&lt;img src=&#34;http://dankleiman.com/img/row_number_rows.png&#34; alt=&#34;BigQuery ROW_NUMBER results&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Results using ARRAY_AGG&lt;/strong&gt;
&lt;img src=&#34;http://dankleiman.com/img/array_agg_rows.png&#34; alt=&#34;BigQuery ARRAY_AGG results&#34;&gt;&lt;/p&gt;

&lt;p&gt;And check out the query plan for the &lt;code&gt;ARRAY_AGG&lt;/code&gt; query:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dankleiman.com/img/array_agg_query_plan.png&#34; alt=&#34;BigQuery ARRAY_AGG query plan&#34;&gt;&lt;/p&gt;

&lt;p&gt;Now, the first 3 stages look pretty similar in terms of row input/output sizes and distribution of wait/read/compute/write. Both plans are working on the sub-select at this point, doing the sum of author comments, grouped by subreddit.&lt;/p&gt;

&lt;p&gt;Stage 4 gets really interesting. The &lt;code&gt;ARRAY_AGG&lt;/code&gt; query takes an row size input of 11.0 M and reduces the output to 19.7K.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dankleiman.com/img/array_agg_stage_4.png&#34; alt=&#34;BigQuery ARRAY_AGG query plan stage 4&#34;&gt;&lt;/p&gt;

&lt;p&gt;In contrast, the &lt;code&gt;ROW_NUMBER&lt;/code&gt; query does a bunch of compute on stage 4, but doesn&amp;rsquo;t reduce the output at all. It&amp;rsquo;s not until the &lt;code&gt;FILTER&lt;/code&gt; in the next stage that we&amp;rsquo;ve cut down on the number of rows in the output (which we already know will still be 10x the &lt;code&gt;ARRAY_AGG&lt;/code&gt; size).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dankleiman.com/img/row_number_stage_4.png&#34; alt=&#34;BigQuery ROW_NUMBER query plan stage 4&#34;&gt;&lt;/p&gt;

&lt;p&gt;It looks like using &lt;code&gt;ROW_NUMBER&lt;/code&gt; forces us to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;run over all the data again to generate our &amp;ldquo;ordered list&amp;rdquo; of comment rankings&lt;/li&gt;
&lt;li&gt;rank everything before we filter out subsets of results we don&amp;rsquo;t ultimately want&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In contrast to the other query strategies, this is starting to seem like wasted compute effort.&lt;/p&gt;

&lt;h2 id=&#34;using-approx-top-sum&#34;&gt;USING APPROX_TOP_SUM&lt;/h2&gt;

&lt;p&gt;One more suggestion I got was to use the &lt;code&gt;APPROX_TOP_&lt;/code&gt; functions. In this case &lt;code&gt;APPROX_TOP_SUM&lt;/code&gt; seemed to do the trick:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT
  subreddit,
  APPROX_TOP_SUM(author,
    total_score,
    10) AS top_scores
FROM (
  SELECT
    subreddit,
    author,
    SUM(score) AS total_score,
    COUNT(DISTINCT author) OVER (PARTITION BY subreddit) AS author_count
  FROM
    `fh-bigquery.reddit_comments.2015_07`
  WHERE
    author NOT IN (&#39;[deleted]&#39;,
      &#39;AutoModerator&#39;)
  GROUP BY
    1,
    2
  HAVING
    SUM(score) &amp;gt; 0 )
WHERE
  author_count &amp;gt; 9
GROUP BY
  1
ORDER BY
  1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#approx_top_sum&#34;&gt;the BigQuery documentation for this function&lt;/a&gt; it says that &lt;code&gt;APPROX_TOP_SUM&lt;/code&gt; &amp;ldquo;returns the approximate top elements of &lt;code&gt;expression&lt;/code&gt;, based on the sum of an assigned &lt;code&gt;weight&lt;/code&gt;. The &lt;code&gt;number&lt;/code&gt; parameter specifies the number of elements returned.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If the weight input is negative or NaN, this function returns an error.&lt;/strong&gt;&amp;ldquo;&lt;/p&gt;

&lt;p&gt;Emphasis added to point out that, of course, since we&amp;rsquo;re dealing with reddit comments, some of them have been downvoted to hell. Negative comment scores will actually break the &lt;code&gt;APPROX_TOP_SUM&lt;/code&gt; function in this case.&lt;/p&gt;

&lt;p&gt;There are over 200 subreddits in this dataset where the top-ranked commenters couldn&amp;rsquo;t break into positive comment counts. I had to add a &lt;code&gt;HAVING&lt;/code&gt; clause to only grab total scores above 0.&lt;/p&gt;

&lt;p&gt;You can see that filter getting applied in stage 3:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dankleiman.com/img/approx_top_sum_stage_3.png&#34; alt=&#34;BigQuery APPROX_TOP_SUM query plan stage 3&#34;&gt;&lt;/p&gt;

&lt;p&gt;Compare that to the previous queries and you can see that stage 3 is where the total amount of data shrinks, thanks to those negative-ranked comments. But even with that caveat, it looks like
&lt;code&gt;APPROX_TOP_SUM&lt;/code&gt; performs similarly to &lt;code&gt;ARRAY_AGG&lt;/code&gt;. In fact, I went back to the &lt;code&gt;ARRAY_AGG&lt;/code&gt; query and adding the same filter on &lt;code&gt;SUM(score)&lt;/code&gt; just to be sure that the inputs and outputs would match at each stage for both queries. They do.&lt;/p&gt;

&lt;p&gt;Still, there are a few differences to point out for &lt;code&gt;APPROX_TOP_SUM&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;why there is an additional stage (highlighted in the screenshot below) that seems to just be shuffling or repartitioning based on some kind of metadata?&lt;/li&gt;
&lt;li&gt;how does the &amp;lsquo;approximate&amp;rsquo; part of &lt;code&gt;APPROX_TOP_SUM&lt;/code&gt; factor into the computation?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://dankleiman.com/img/approx_top_sum_query_plan.png&#34; alt=&#34;BigQuery APPROX_TOP_SUM query plan&#34;&gt;&lt;/p&gt;

&lt;p&gt;Other than that, it seems like both of these options deliver more concise results than the initial &lt;code&gt;ROW_NUMBER&lt;/code&gt; solution. I&amp;rsquo;m looking forward to understanding the query explanation tools even better, so that in the future I can use them to diagnose my less efficient queries!&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Top N Per Group in BigQuery</title>
      <link>http://dankleiman.com/2017/10/30/top-n-per-group-in-bigquery</link>
      <pubDate>Mon, 30 Oct 2017 19:29:32 -0400</pubDate>
      
      <guid>http://dankleiman.com/2017/10/30/top-n-per-group-in-bigquery</guid>
      <description>&lt;p&gt;&lt;strong&gt;EDIT:&lt;/strong&gt; After I posted this initially, I got some &lt;a href=&#34;https://twitter.com/Dan_Kleiman/status/925921880397287425&#34;&gt;great&lt;/a&gt; &lt;a href=&#34;https://www.reddit.com/r/bigquery/comments/7aecfe/top_n_per_group_in_bigquery/&#34;&gt;feedback&lt;/a&gt;, so I wrote a follow-up post &lt;a href=&#34;http://dankleiman.com/2017/11/07/more-efficient-solutions-to-the-top-n-per-group-problem/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post, we are going to explore a strategy for collecting the &lt;strong&gt;Top N results per Group&lt;/strong&gt; over a mixed dataset, all in a single query.&lt;/p&gt;

&lt;p&gt;I stumbled onto this solution the other day, mostly driven by the fear that I was re-scanning my BigQuery data too often. At the time, the only way I knew how to look at a Top 10 list of a subset of the data was to add a &lt;code&gt;WHERE&lt;/code&gt; clause limiting the whole data set to a single group and combine with &lt;code&gt;ORDER BY&lt;/code&gt; and &lt;code&gt;LIMIT&lt;/code&gt; clauses.&lt;/p&gt;

&lt;p&gt;For each group, I would just modify the &lt;code&gt;WHERE&lt;/code&gt; clause, rescan all the data, and get new results. I thought there had to be an easier way to get the same ordered subset for any particular group in the data, all at once.&lt;/p&gt;

&lt;p&gt;It turns out, there is a much more efficient way to solve this problem.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h2 id=&#34;reddit-top-10-users-by-comment-score-for-july-2015&#34;&gt;Reddit Top 10 Users By Comment Score for July, 2015&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s start with an example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT
  author,
  sum(score) as comment_score
FROM
  `fh-bigquery.reddit_comments.2015_07`
WHERE author NOT IN (&#39;[deleted]&#39;, &#39;AutoModerator&#39;)  
GROUP BY 1
ORDER BY 2 DESC
LIMIT 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;m leaving out all query results in this post because it&amp;rsquo;s a &lt;a href=&#34;https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_07?tab=preview&#34;&gt;public dataset&lt;/a&gt; and you should totally go run the queries to see for yourself!&lt;/p&gt;

&lt;p&gt;In the most straightforward way possible, we&amp;rsquo;re summing up the comment score by author, ordering by highest score, and taking the first ten results.&lt;/p&gt;

&lt;p&gt;While top commenters across all of reddit might be meaningful, you probably want to look at a specific subreddit.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT
  author,
  SUM(score) AS comment_score
FROM
  `fh-bigquery.reddit_comments.2015_07`
WHERE
  author NOT IN (&#39;[deleted]&#39;, &#39;AutoModerator&#39;)
  AND subreddit = &#39;webdev&#39;
GROUP BY
  1
ORDER BY
  2 DESC
LIMIT 10
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;By adding another filter to your &lt;code&gt;WHERE&lt;/code&gt; clause you can now see relevant top commenters in a single subreddit.&lt;/p&gt;

&lt;p&gt;This is what I was doing initially to inspect the dataset. I kept swapping out the subreddit in the &lt;code&gt;WHERE&lt;/code&gt; clause and running the query again to view top commenters in different subreddits.&lt;/p&gt;

&lt;p&gt;That was an okay first pass at understanding the data, but it&amp;rsquo;s wasteful to re-scan the whole table every time to pull a single Top 10 result set. The better approach would be to get the Top 10 results for each subreddit all at once and store the results to its own table that you can then query for a single subreddit as much as you want.&lt;/p&gt;

&lt;p&gt;Also, if you ever wanted to build a data visualization tool off this view into the comment data, you wouldn&amp;rsquo;t want to compute scores and rankings each time. So either way, working towards a single query makes a lot of sense.&lt;/p&gt;

&lt;h2 id=&#34;top-10-per-subreddit&#34;&gt;Top 10 per Subreddit&lt;/h2&gt;

&lt;p&gt;So far we have a pretty easy way to either get the Top 10 of the whole set or the Top 10 of a subset of the data.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s look at a technique for getting the Top 10 of each subset all at once. Thanks to this &lt;a href=&#34;https://stackoverflow.com/questions/44680464/get-top-n-records-for-each-group-of-grouped-results-with-bigquery-standard-sql?answertab=votes#tab-top&#34;&gt;stackoverflow post&lt;/a&gt;, I had a solution, but I wanted to understand how to get to that solution from the &amp;ldquo;one Top 10 at a time&amp;rdquo; approach.&lt;/p&gt;

&lt;h3 id=&#34;row-number&#34;&gt;ROW_NUMBER()&lt;/h3&gt;

&lt;p&gt;First, we need a way to order the commenters by score within each group. The key here is using the analytic function &lt;code&gt;ROW_NUMBER()&lt;/code&gt;. &lt;a href=&#34;https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#analytic-functions&#34;&gt;In databases, an analytic function is a function that computes aggregate values over a group of rows.&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT
  subreddit,
  author,
  SUM(score) AS total_score,
  ROW_NUMBER() OVER (PARTITION BY subreddit ORDER BY SUM(score) DESC) AS comment_rank
FROM
  `fh-bigquery.reddit_comments.2015_07`
WHERE
  author NOT IN (&#39;[deleted]&#39;,
    &#39;AutoModerator&#39;)
GROUP BY
  1,
  2
ORDER BY
  1,4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This query will give us every author who commented in every subreddit with their aggregate comment score, &lt;em&gt;ranked by their comment score within the specific subreddit&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We used the &lt;code&gt;ROW_NUMBER()&lt;/code&gt; function to create an ordered list of scores, highest to lowest, by partitioning the data in a way that looks a lot like our second query, where we ranked comment scores from a single subreddit.&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;re almost there!&lt;/p&gt;

&lt;p&gt;We&amp;rsquo;ve got relative rank of each author within each subreddit in one query, but what we&amp;rsquo;re missing is our Top 10 List for each subreddit. The query above still gives us all authors for all ranks.&lt;/p&gt;

&lt;h3 id=&#34;subquery-ftw&#34;&gt;Subquery FTW&lt;/h3&gt;

&lt;p&gt;Unfortunately, we can&amp;rsquo;t just put a &lt;code&gt;WHERE&lt;/code&gt; clause like &lt;code&gt;comment_rank &amp;lt;= 10&lt;/code&gt; into the query or use a &lt;code&gt;HAVING&lt;/code&gt; constraint on the &lt;code&gt;GROUP BY&lt;/code&gt;. Instead, we need to do a subquery to select from the ranked data:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SELECT
  subreddit,
  author,
  author_count,
  total_score,
  comment_rank
FROM (
  SELECT
    subreddit,
    author,
    SUM(score) AS total_score,
    ROW_NUMBER() OVER (PARTITION BY subreddit ORDER BY SUM(score) DESC) AS comment_rank,
    COUNT(DISTINCT author) OVER (PARTITION BY subreddit) AS author_count
  FROM
    `fh-bigquery.reddit_comments.2015_07`
  WHERE
    author NOT IN (&#39;[deleted]&#39;,&#39;AutoModerator&#39;)
  GROUP BY
    1,2
    )
WHERE
  comment_rank &amp;lt;= 10
  AND author_count &amp;gt; 9
ORDER BY
  1,5
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We are pulling all results where the &lt;code&gt;comment_rank &amp;lt;= 10&lt;/code&gt;, meaning positions 1-10 in each subreddit by aggregate comment score. And just as a way of cleaning up the data even more, we added in the &lt;code&gt;author_count&lt;/code&gt; column so that we can ensure that each subset has at least 10 authors &amp;ndash; we&amp;rsquo;ll get a full Top 10 for each of the subreddits in our result.&lt;/p&gt;

&lt;h3 id=&#34;verification&#34;&gt;Verification&lt;/h3&gt;

&lt;p&gt;To verify this data, you can pick out any subreddit in the result set and compare it to the single subreddit query (the second query in this post) above. Spot any differences? We&amp;rsquo;ll leave it as an exercise to the reader to figure out a tie-breaker strategy if we really wanted to ensure the same Top 10 results every time.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Don&#39;t Blow Your BigQuery Budget on Unknown Data!</title>
      <link>http://dankleiman.com/2017/10/06/dont-blow-your-bigquery-budget-on-unknown-data</link>
      <pubDate>Fri, 06 Oct 2017 14:55:37 -0400</pubDate>
      
      <guid>http://dankleiman.com/2017/10/06/dont-blow-your-bigquery-budget-on-unknown-data</guid>
      <description>&lt;p&gt;It&amp;rsquo;s easy to blow your BigQuery budget when you are exploring a new data set. Because you&amp;rsquo;re billed for the amount of data scanned, not the ultimate result set, when you don&amp;rsquo;t know what you&amp;rsquo;re looking  for, you can end up with wasteful queries.&lt;/p&gt;

&lt;p&gt;In this post, I&amp;rsquo;m going to share some tips for more efficiently scanning data in BigQuery when you don&amp;rsquo;t quite know what you need.
&lt;/p&gt;

&lt;p&gt;To ground some of these questions in an example, at work, we often investigate unknowns in our data, especially where we integrate with third parties. Recently, we were reconciling some data with a third party that posts us back about in-app events. They had about 50,000 events for one month, and our logs showed that we processed 10,000. What happened to the delta? Who had the more accurate number?&lt;/p&gt;

&lt;p&gt;The table in BigQuery that holds these particular event logs for us gets about 1 billion postbacks a day and the period in question was a whole month. So, without blowing my whole BigQuery budget, I had to poke at this large data set and explain how those 50k needles moved through our haystack of 30 billion requests.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re new to BigQuery, and drawing on your past experience with other databases, it&amp;rsquo;s tempting to want to &lt;code&gt;SELECT * FROM whole_month_of_data&lt;/code&gt;, export it to Excel, and do a &lt;code&gt;VLOOKUP&lt;/code&gt; to see if their data is there&amp;hellip;but as you&amp;rsquo;re about to learn, that is the 100% wrong approach to efficiently working with data in BigQuery.&lt;/p&gt;

&lt;p&gt;Now, I can&amp;rsquo;t share our actual queries and data with you for this particular case, but fortunately, &lt;a href=&#34;https://cloud.google.com/bigquery/public-data/&#34;&gt;Google makes public datasets available&lt;/a&gt; to anyone using BigQuery. And you can start to play around on their free tier, provided you follow these tips to stay under the free allocation.&lt;/p&gt;

&lt;p&gt;I used all these same techiniques to ultimately piece together a clear explanation of how the event data moved through our applications&amp;hellip;and I only blew my daily BigQuery budget on the first day of this investigation.&lt;/p&gt;

&lt;h2 id=&#34;never-use-select&#34;&gt;Never Use Select *&lt;/h2&gt;

&lt;p&gt;So, the first thing you should do when you open up a new dataset, is NEVER USE &lt;code&gt;SELECT *&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re used to working with smaller row-based relationational datastores, like MySQL or Postgres, then &lt;code&gt;SELECT *&lt;/code&gt; is a handy way to get a feel for what&amp;rsquo;s stored in your tables.&lt;/p&gt;

&lt;p&gt;However, in a column-based store like BigQuery, &lt;code&gt;SELECT *&lt;/code&gt; is much less efficient. Instead, you want to explore column-by-column.&lt;/p&gt;

&lt;p&gt;Here&amp;rsquo;s a good explanation of the difference &lt;a href=&#34;http://docs.aws.amazon.com/redshift/latest/dg/c_columnar_storage_disk_mem_mgmnt.html&#34;&gt;between row-based and column-based datastores&lt;/a&gt;. Basically, in a row store, the data for each row is stored continguously in memory on disk, so when you &lt;code&gt;SELECT *&lt;/code&gt; and pull out the whole row, you&amp;rsquo;re reaching for adjacent memory that is more easily pulled all at once.&lt;/p&gt;

&lt;p&gt;In a column-based store, the allocation is inverted, so the whole column of data is stored together, not the row. Every time you have to pull a new column to build your row, you&amp;rsquo;re reaching somewhere else on disk, which is less efficient.&lt;/p&gt;

&lt;p&gt;Pulling an entire column is going to be cheaper than pulling all columns, even if you do something like &lt;code&gt;SELECT * FROM whatever LIMIT 1&lt;/code&gt; because each of the non-contiguous columns has to be scanned to get the data you need.&lt;/p&gt;

&lt;h2 id=&#34;handy-preview-mode&#34;&gt;Handy Preview Mode&lt;/h2&gt;

&lt;p&gt;If you need to explore an unknown data set, you can still get a feel for what&amp;rsquo;s in each column and the different characteristics of the data by using BigQuery&amp;rsquo;s built in &amp;ldquo;preview&amp;rdquo; function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dankleiman.com/images/bigquery_preview_mode.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m using &lt;a href=&#34;https://bigquery.cloud.google.com/table/fh-bigquery:reddit_comments.2015_01?tab=preview&#34;&gt;this publicly available dump of Reddit comments&lt;/a&gt; for all the screenshots in this post.&lt;/p&gt;

&lt;p&gt;Use preview to refine the questions you need to answer with the data and understand what data is available. When you find a potentially useful column, just query this column, grouping values or filtering results to understand what&amp;rsquo;s there more thoroughly.&lt;/p&gt;

&lt;p&gt;But remember, even here, you don&amp;rsquo;t want to be crawling the whole data set you&amp;rsquo;re ultimately interested in each time. Instead, try to limit it to a single shard of data &amp;ndash; in my work example above, that would have been a single day of data, even though I was ultimately going to have to scan a whole month.&lt;/p&gt;

&lt;h2 id=&#34;run-your-big-query-once&#34;&gt;Run Your Big Query Once&lt;/h2&gt;

&lt;p&gt;Once you&amp;rsquo;ve identified the columns and clauses you need, run the &amp;ldquo;big query&amp;rdquo; once and store it to a temp table.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dankleiman.com/images/bigquery_temp_table.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;Now, instead of querying against billions of rows each time, you will be working with a subset of data. Even if it&amp;rsquo;s a few million rows, it will still be more efficient.&lt;/p&gt;

&lt;h2 id=&#34;import-external-data&#34;&gt;Import External Data&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;re comparing BigQuery data to external data, you might be tempted to export your BigQuery data at this point and try to compare data in spreadsheets. Don&amp;rsquo;t!&lt;/p&gt;

&lt;p&gt;Instead, you can import data into BigQuery as a temp table and use the auto-detect schema feature. BigQuery will build you a temp table with all the columns and rows from your spreadsheet.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://dankleiman.com/images/bigquery_import_data.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;Now you can use your temp table or any other data set to write SQL against your external data. You can even join data across BigQuery projects &amp;ndash; the project name is just part of the namespace that identifies each table.&lt;/p&gt;

&lt;h2 id=&#34;now-go-query&#34;&gt;Now Go Query&lt;/h2&gt;

&lt;p&gt;I know when I started working with data in BigQuery, I was always afraid I was going to scan too much data, or write bad SQL. I probably still do that on a regular basis, but hopefully, by following these guidelines, I&amp;rsquo;ve become a little smarter about how I tackle an unknown data problem.&lt;/p&gt;

&lt;p&gt;And I hope you found at least one good tip in this post to go and use too!&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>